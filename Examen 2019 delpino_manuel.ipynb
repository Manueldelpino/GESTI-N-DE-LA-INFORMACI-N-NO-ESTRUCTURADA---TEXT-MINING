{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examen Gestión de la Información no estructurada (TEXT MINING) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manuel del Pino Guerrero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos librerías necesarias para realizar el examen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realice un pre-procesamiento del siguiente texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = '''María Zambrano (1904, Vélez Málaga-1991, Madrid) figura en la cima de la filosofía del pensamiento español. \n",
    "Desde 1909 a 1924 vivió en Segovia, donde conoció a Antonio Machado. Discípula predilecta de Ortega y Gasset, asistió también \n",
    "a clases de García Morente, Manuel Bartolomé Cossío y Xavier Zubiri. Empezó a publicar artículos en 1928 y su primer libro, \n",
    "Horizonte del liberalimo data de 1930. Ya en la República, participó en las Misiones Pedagógicas. Su exilio la llevó por Chile, \n",
    "México, Cuba, Puerto Rico, Roma, París y Suiza.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El idioma del texto es: es\n"
     ]
    }
   ],
   "source": [
    "print('El idioma del texto es:', textacy.text_utils.detect_language(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) En la anterior celda podemos comprobar como a través del comando \"textacy.text_utils.detect_language()\", hemos obtenido un print en la terminal que nos indica que efectivamente el idioma del texto seleccionado es español \"es\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) A continuación vamos a crear el tokenizador necesario para separar el texto en frases, sacando también dichas frases por separado para comprobar que funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['María Zambrano (1904, Vélez Málaga-1991, Madrid) figura en la cima de la filosofía del pensamiento español.',\n",
       " 'Desde 1909 a 1924 vivió en Segovia, donde conoció a Antonio Machado.',\n",
       " 'Discípula predilecta de Ortega y Gasset, asistió también \\na clases de García Morente, Manuel Bartolomé Cossío y Xavier Zubiri.',\n",
       " 'Empezó a publicar artículos en 1928 y su primer libro, \\nHorizonte del liberalimo data de 1930.',\n",
       " 'Ya en la República, participó en las Misiones Pedagógicas.',\n",
       " 'Su exilio la llevó por Chile, \\nMéxico, Cuba, Puerto Rico, Roma, París y Suiza.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "frases = tokenizador.tokenize(texto)\n",
    "frases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Ahora vamos a dividir el documento en 20 tokens por ejemplo, e imprimirlos utilizando \"nltk.word_tokenize()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['María',\n",
       " 'Zambrano',\n",
       " '(',\n",
       " '1904',\n",
       " ',',\n",
       " 'Vélez',\n",
       " 'Málaga-1991',\n",
       " ',',\n",
       " 'Madrid',\n",
       " ')',\n",
       " 'figura',\n",
       " 'en',\n",
       " 'la',\n",
       " 'cima',\n",
       " 'de',\n",
       " 'la',\n",
       " 'filosofía',\n",
       " 'del',\n",
       " 'pensamiento',\n",
       " 'español']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_español = nltk.word_tokenize(texto)\n",
    "tokens_español[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extraiga y escriba por pantalla las formas normales y la categoría gramatical de cada una de las palabras del texto anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a obtener las formas normales (lemas) de los anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "María\n",
      "Zambrano\n",
      "(\n",
      "1904\n",
      ",\n",
      "Vélez\n",
      "Málaga-1991\n",
      ",\n",
      "Madrid\n",
      ")\n",
      "figura\n",
      "en\n",
      "la\n",
      "cima\n",
      "de\n",
      "la\n",
      "filosofía\n",
      "del\n",
      "pensamiento\n",
      "español\n",
      ".\n",
      "Desde\n",
      "1909\n",
      "a\n",
      "1924\n",
      "vivió\n",
      "en\n",
      "Segovia\n",
      ",\n",
      "donde\n",
      "conoció\n",
      "a\n",
      "Antonio\n",
      "Machado\n",
      ".\n",
      "Discípula\n",
      "predilecta\n",
      "de\n",
      "Ortega\n",
      "y\n",
      "Gasset\n",
      ",\n",
      "asistió\n",
      "también\n",
      "a\n",
      "clases\n",
      "de\n",
      "García\n",
      "Morente\n",
      ",\n",
      "Manuel\n",
      "Bartolomé\n",
      "Cossío\n",
      "y\n",
      "Xavier\n",
      "Zubiri\n",
      ".\n",
      "Empezó\n",
      "a\n",
      "publicar\n",
      "artículos\n",
      "en\n",
      "1928\n",
      "y\n",
      "su\n",
      "primer\n",
      "libro\n",
      ",\n",
      "Horizonte\n",
      "del\n",
      "liberalimo\n",
      "data\n",
      "de\n",
      "1930\n",
      ".\n",
      "Ya\n",
      "en\n",
      "la\n",
      "República\n",
      ",\n",
      "participó\n",
      "en\n",
      "la\n",
      "Misiones\n",
      "Pedagógicas\n",
      ".\n",
      "Su\n",
      "exilio\n",
      "la\n",
      "llevó\n",
      "por\n",
      "Chile\n",
      ",\n",
      "México\n",
      ",\n",
      "Cuba\n",
      ",\n",
      "Puerto\n",
      "Rico\n",
      ",\n",
      "Roma\n",
      ",\n",
      "París\n",
      "y\n",
      "Suiza\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "lematizador = WordNetLemmatizer()\n",
    "for palabra in tokens_español:\n",
    "    print(lematizador.lemmatize(palabra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Realice una extracción de entidades del texto anterior. Deben aparecer al menos entidades de tipo persona, localización y organización. Comente los resultados obtenidos. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #Cargo esta librería para dar mejor soporte de extracción de entidades en textos en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['María Zambrano (1904, Vélez Málaga-1991, Madrid) figura en la cima de la filosofía del pensamiento español.',\n",
       " 'Desde 1909 a 1924 vivió en Segovia, donde conoció a Antonio Machado.',\n",
       " 'Discípula predilecta de Ortega y Gasset, asistió también \\na clases de García Morente, Manuel Bartolomé Cossío y Xavier Zubiri.',\n",
       " 'Empezó a publicar artículos en 1928 y su primer libro, \\nHorizonte del liberalimo data de 1930.',\n",
       " 'Ya en la República, participó en las Misiones Pedagógicas.',\n",
       " 'Su exilio la llevó por Chile, \\nMéxico, Cuba, Puerto Rico, Roma, París y Suiza.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases = nltk.sent_tokenize(texto)\n",
    "frases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comienzo la extracción de entidades del texto proporcionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON María/NNP)\n",
      "  (ORGANIZATION Zambrano/NNP)\n",
      "  (/(\n",
      "  1904/CD\n",
      "  ,/,\n",
      "  (PERSON Vélez/NNP Málaga-1991/NNP)\n",
      "  ,/,\n",
      "  (GPE Madrid/NNP)\n",
      "  )/)\n",
      "  figura/NN\n",
      "  en/IN\n",
      "  la/FW\n",
      "  cima/FW\n",
      "  de/FW\n",
      "  la/FW\n",
      "  filosofía/FW\n",
      "  del/FW\n",
      "  pensamiento/NN\n",
      "  español/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Desde/NNP\n",
      "  1909/CD\n",
      "  a/DT\n",
      "  1924/CD\n",
      "  vivió/NN\n",
      "  en/FW\n",
      "  (GPE Segovia/NNP)\n",
      "  ,/,\n",
      "  donde/NN\n",
      "  conoció/NN\n",
      "  a/DT\n",
      "  (ORGANIZATION Antonio/NNP Machado/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  (GPE Discípula/NNP)\n",
      "  predilecta/NN\n",
      "  de/FW\n",
      "  (GPE Ortega/NNP)\n",
      "  y/NN\n",
      "  (PERSON Gasset/NNP)\n",
      "  ,/,\n",
      "  asistió/VBZ\n",
      "  también/VBP\n",
      "  a/DT\n",
      "  clases/NNS\n",
      "  de/IN\n",
      "  (PERSON García/NNP Morente/NNP)\n",
      "  ,/,\n",
      "  (PERSON Manuel/NNP Bartolomé/NNP Cossío/NNP)\n",
      "  y/NNP\n",
      "  Xavier/NNP\n",
      "  Zubiri/NNP\n",
      "  ./.)\n",
      "(S\n",
      "  Empezó/VB\n",
      "  a/DT\n",
      "  publicar/NN\n",
      "  artículos/NN\n",
      "  en/IN\n",
      "  1928/CD\n",
      "  y/NN\n",
      "  su/NN\n",
      "  primer/NN\n",
      "  libro/NN\n",
      "  ,/,\n",
      "  (PERSON Horizonte/NNP)\n",
      "  del/FW\n",
      "  liberalimo/NN\n",
      "  data/NNS\n",
      "  de/IN\n",
      "  1930/CD\n",
      "  ./.)\n",
      "(S\n",
      "  Ya/NNP\n",
      "  en/CC\n",
      "  la/NN\n",
      "  (ORGANIZATION República/NNP)\n",
      "  ,/,\n",
      "  participó/NN\n",
      "  en/FW\n",
      "  las/FW\n",
      "  (PERSON Misiones/NNP Pedagógicas/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  (GPE Su/NNP)\n",
      "  exilio/NN\n",
      "  la/NN\n",
      "  llevó/FW\n",
      "  por/NN\n",
      "  Chile/NNP\n",
      "  ,/,\n",
      "  (GPE México/NNP)\n",
      "  ,/,\n",
      "  (GPE Cuba/NNP)\n",
      "  ,/,\n",
      "  (PERSON Puerto/NNP Rico/NNP)\n",
      "  ,/,\n",
      "  (GPE Roma/NNP)\n",
      "  ,/,\n",
      "  (PERSON París/NNP)\n",
      "  y/NNP\n",
      "  Suiza/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "frases_token = [nltk.word_tokenize(frase) for frase in frases] \n",
    "\n",
    "frases_tagged = [nltk.pos_tag(frase) for frase in frases_token]\n",
    "\n",
    "for fras in frases_tagged:\n",
    "    print(nltk.ne_chunk(fras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como podemos observar trás realizar la extracción de entidades previa, se puede ver que al tratarse de un texto español utilizamos la librería previamente cargada spaCy, ya que la librería NLTK está muy preparada para desarrollar y analizar corpus de documentos en idioma Inglés, por lo que siguiendo diferentes recomendaciones vamos a usar la librería spaCy para a continuación llevar a cabo nuestro análisis de un txt en idioma español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las confusiones y errores son mayores ya que identifica verbos como personas y viceversa, por lo que diría que el grado de acierto semántico es mucho menor en el caso español. Otor ejemplo es que identifica la palabra \"Puerto\" como PERSONA en vez de lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple está buscando comprar una startup del Reino Unido por mil millones de dólares\n"
     ]
    }
   ],
   "source": [
    "print (doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "está AUX aux\n",
      "buscando VERB ROOT\n",
      "comprar VERB xcomp\n",
      "una DET det\n",
      "startup NOUN obj\n",
      "del ADP case\n",
      "Reino PROPN nmod\n",
      "Unido PROPN flat\n",
      "por ADP case\n",
      "mil NUM nummod\n",
      "millones NOUN nmod\n",
      "de ADP case\n",
      "dólares NOUN nmod\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print (token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora una vez comporbamos que la librería spaCy funciona correctamente, vamos a tratar de extraer entidades de nuestro texto en español con ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_spanish = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['María Zambrano', '1904', 'Vélez Málaga-1991', 'Madrid', 'la cima', 'la filosofía', 'pensamiento', '1909', '1924', 'Segovia', 'donde', 'Antonio Machado', 'predilecta', 'Ortega', 'Gasset', 'clases', 'García Morente', 'Manuel Bartolomé Cossío', 'Xavier Zubiri', 'artículos', '1928', 'su primer libro', 'Horizonte del liberalimo', '1930', 'la República', 'las Misiones Pedagógicas', 'Su exilio', 'la', 'Chile', 'México', 'Cuba', 'Puerto Rico', 'Roma', 'París', 'Suiza']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc_spanish.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['figurar', 'vivir', 'conocer', 'asistir', 'publicar', 'datar', 'participar', 'llevar']\n"
     ]
    }
   ],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc_spanish if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar entidades, frases y otros conceptos hacemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "María Zambrano PER\n",
      "Vélez Málaga-1991 PER\n",
      "Madrid ORG\n",
      "\n",
      " ORG\n",
      "Segovia LOC\n",
      "Antonio Machado PER\n",
      "Ortega y Gasset PER\n",
      "\n",
      " PER\n",
      "García Morente PER\n",
      "Manuel Bartolomé Cossío PER\n",
      "Xavier Zubiri PER\n",
      "\n",
      "Horizonte MISC\n",
      "Ya en la República MISC\n",
      "Misiones Pedagógicas LOC\n",
      "Chile LOC\n",
      "\n",
      "México ORG\n",
      "Cuba LOC\n",
      "Puerto Rico LOC\n",
      "Roma LOC\n",
      "París LOC\n",
      "Suiza LOC\n"
     ]
    }
   ],
   "source": [
    "for entity in doc_spanish.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como podemos comprobar con estas últimas acciones y siguiendo las instrucciones del profesor Francisco Izquierdo, la librería spaCy ajusta mucho mejor semánticamente la extracción de entidades de nuestro Corpus en cuestión dando lugar a pocas confusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extraiga los ficheros que se encuentran en el archivo docus.zip en un directorio. Léalos desde su notebook y realice las tareas de pre-procesamiento que considere necesarias para abordar una extracción de tópicos. Genere 3 tópicos y saque por pantalla la lista con los 5 términos más frecuentes de cada tópico. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedo a cargar los paquetes y librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "#from stopwords import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "import nltk, re, pprint, spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Knowhow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargo library stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# De esta manera importo de manera más eficiente la librería stopwords que cargaré luego.\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construímos el Corpus dándole formato encoding utf-8 a cada uno de los 15 textos seleccionados. ya que de lo contrario no sería posible\n",
    "# continuar con el análisis LDA.\n",
    "\n",
    "\n",
    "docs_dir = \"C:/Users/Knowhow/Desktop/CUNEF/GESTION DE LA INFORMACIÓN NO ESTRUCTURADA/EXAMEN 2019 manuel_delpino/docus\"\n",
    "documentos = PlaintextCorpusReader(docs_dir, '.*txt') \n",
    "corpus = [] # Preparo el conjunto de documentos (Corpus con los 15 txt de noticias que he escogido) para realizar un análisis de tipo LDA.\n",
    "\n",
    "for fileid in documentos.fileids():\n",
    "    documento = documentos.raw(fileids=fileid)\n",
    "    corpus.append(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Terminadas las semifinales de la liga ACB de baloncesto, los play-off arrancarán el próximo sábado en Madrid,', 'Una canasta en los últimos momentos dio un resultado inesperado en las finales de la temporada de baloncesto en Estados Unidos', 'La ACB se reunirá para estudiar la conveniencia de aplicar en las canchas de baloncesto las reglas de la NBA, entre ellas alejar a mayor distancia de la canasta la línea de triple. ', 'El mercado de fichajes en las canchas de baloncesto para la próxima temporada ya está en auge. Los diferentes equipos se intentan reforzar para una temporada que presentará cambios en el número de partidos.', 'Polémica en la liga de baloncesto griega. Los dos principales equipos, Panathinaikos y Olympiacos han tenido una temporada con duros enfrentamientos, hasta el punto de que Olympiacos se ha negado a jugar el derbi, y el Panathinaikos gana por 0-20.', 'Vargas Llosa anuncia un libro sobre el golpe militar auspiciado por la CIA en Guatemala en 1954', 'El escritor peruano anunció ayer que este año publicará una novela cuyo título hace alusión «a una cita de Santa Teresa de Ávila»: «Tiempos recios», con edición de Alfaguara.', 'Los libros que José Luis Garcías compró en su visita privada a la Feria del Libro de Madrid', 'Entre lo libros más vendidos de la presente Feria se encuentra sin duda el último ejemplar firmado por Santiago Posteguillo.', 'Muere Agustina Bessa-Luís, una de las grandes escritoras de Portugal. Se hizo conocida para el gran público luso a raíz de su recopilación de las costumbres del Portugal de los siglos XIX y XX, recogidas en su libro Sibila.\\n', '¿Está tu ordenador afectado por Spectre y Meltdown? Lo que necesitas saber', 'Nuevos periféricos para los ordenadores de alta gama de Apple.', '¿Cuál es el ordenador portátil más potente? Comparativa de Surface Book 2 de Microsoft frente a las ofertas de Apple', 'Dell quiere atacar las principales preocupaciones empresariales de TI con sus nuevos modelos de ordenadores de sobremesa.', '¿Tablet u ordenador portátil de última generación? Analizamos las principales diferencias entre dos mundos que se acercan.']\n"
     ]
    }
   ],
   "source": [
    "doc_list=[]\n",
    "docs_dir = \"C:/Users/Knowhow/Desktop/CUNEF/GESTION DE LA INFORMACIÓN NO ESTRUCTURADA/EXAMEN 2019 manuel_delpino/docus\"\n",
    "documentos = PlaintextCorpusReader(docs_dir, '.*txt')\n",
    "\n",
    "for fileid in documentos.fileids():\n",
    "    doc_list.append(documentos.raw(fileids=fileid))\n",
    "\n",
    "print(doc_list[:15]) #Aquí estoy imprimiendo los 15 primeros documentos del CORPUS total de 15 txt con diferentes tópicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Obtenga el resultado gráfico del análisis mediante la librería pyLDAvis. Comente los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vectorizamos el corpus\n",
    "# 2. LatentDirichletAllocation actúa sobre una matriz términos - documentos\n",
    "# 3. CountVectorizer precismente transforma un conjunto de documentos (corpus) en una matriz \n",
    "    \n",
    "cv = CountVectorizer(strip_accents='unicode', analyzer='word')\n",
    "Xc = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos LDA\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3, learning_method='online', max_iter=25)\n",
    "Xl = lda.fit_transform(Xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0\n",
      "griega\n",
      "aplicar\n",
      "costumbres\n",
      "gana\n",
      "estudiar\n",
      "\n",
      "Topic 1\n",
      "ti\n",
      "empresariales\n",
      "sus\n",
      "atacar\n",
      "preocupaciones\n",
      "\n",
      "Topic 2\n",
      "spectre\n",
      "gana\n",
      "jugar\n",
      "una\n",
      "dos\n",
      "Document 0:\n",
      "Terminadas las semifinales de la liga ACB de baloncesto, los play-off arrancarán el próximo sábado en Madrid,\n",
      "[0.01800615 0.96358267 0.01841118]\n",
      "Document 7:\n",
      "Los libros que José Luis Garcías compró en su visita privada a la Feria del Libro de Madrid\n",
      "[0.01886556 0.35189911 0.62923533]\n",
      "[[0.00954105 0.73577327 0.25468568]]\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las 5 palabras por tópico\n",
    "Mwts_lda = np.argsort(lda.components_, axis=1)[::-1]\n",
    "\n",
    "for t in range(3):\n",
    "    print('\\nTopic ' + str(t))\n",
    "    for i in range(5):\n",
    "        print(cv.get_feature_names()[Mwts_lda[t, i]])\n",
    "\n",
    "# Test the model with new document\n",
    "print('Document 0:')\n",
    "print(corpus[0])\n",
    "print(Xl[0])\n",
    "\n",
    "print('Document 7:')\n",
    "print(corpus[7])\n",
    "print(Xl[7])\n",
    "\n",
    "test_doc = corpus[0] + ' ' + corpus[7]\n",
    "y_test = lda.transform(cv.transform([test_doc]))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora voy a cargar la lista de palabras vacías en español con 'stopwords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "{'hubiesen', 'ella', 'sí', 'seréis', 'sería', 'nosotras', 'teníais', 'estén', 'hay', 'estés', 'estados', 'se', 'pero', 'estado', 'sois', 'ellos', 'soy', 'tendrían', 'estuvisteis', 'hubo', 'suyo', 'tengáis', 'las', 'más', 'tuve', 'tenidas', 'tenía', 'algunos', 'tenemos', 'mis', 'mía', 'estábamos', 'tuyas', 'estamos', 'serán', 'tu', 'estarías', 'estaría', 'o', 'todos', 'erais', 'fueron', 'teníamos', 'estarán', 'tendría', 'estuviesen', 'tuvierais', 'estarían', 'una', 'estaba', 'estuviésemos', 'serías', 'tuyos', 'tuvimos', 'estuviste', 'tendríais', 'hayan', 'fuera', 'poco', 'serás', 'algo', 'era', 'tengan', 'porque', 'he', 'hubiera', 'esa', 'antes', 'ya', 'suyos', 'hubiésemos', 'los', 'seamos', 'tuviste', 'cual', 'estáis', 'están', 'tengamos', 'estadas', 'ni', 'me', 'estando', 'sentid', 'estas', 'estad', 'habidas', 'desde', 'tienen', 'tendremos', 'les', 'hubiéramos', 'seríais', 'esté', 'hubieseis', 'tendrás', 'estuvimos', 'habíamos', 'tuviéramos', 'estuviera', 'estará', 'fui', 'eres', 'le', 'también', 'tanto', 'durante', 'estuvo', 'como', 'haya', 'habría', 'fuésemos', 'habré', 'míos', 'yo', 'tuviesen', 'al', 'estuviese', 'tengas', 'estaré', 'vuestras', 'estaremos', 'muchos', 'habrían', 'tenga', 'vuestro', 'habéis', 'tuvieron', 'tuvieses', 'un', 'nuestra', 'esos', 'nuestros', 'habidos', 'entre', 'esto', 'tú', 'vuestra', 'hayáis', 'tenías', 'estuvieran', 'hubieron', 'sus', 'todo', 'ha', 'tuvo', 'habrá', 'tuvieras', 'cuando', 'mí', 'estabais', 'seré', 'tenidos', 'vosostros', 'habíais', 'sentidos', 'habríamos', 'vuestros', 'hube', 'tuvisteis', 'será', 'habremos', 'tuviera', 'os', 'esas', 'habían', 'y', 'contra', 'tus', 'hubieses', 'te', 'tuviésemos', 'otras', 'estás', 'tenida', 'has', 'sin', 'estabas', 'otro', 'tengo', 'hubiste', 'tendríamos', 'quienes', 'es', 'tuvieseis', 'lo', 'vosostras', 'la', 'no', 'algunas', 'unos', 'a', 'habrás', 'tened', 'nosotros', 'seremos', 'en', 'hubisteis', 'estaban', 'estuvierais', 'han', 'habréis', 'fuisteis', 'estemos', 'nuestro', 'ti', 'nada', 'estada', 'habida', 'otra', 'tenido', 'fue', 'sentida', 'siente', 'con', 'tuyo', 'estaríamos', 'estar', 'nuestras', 'está', 'hubieras', 'éramos', 'fuiste', 'fuerais', 'seas', 'ellas', 'muy', 'hubimos', 'estéis', 'serían', 'estaréis', 'habrías', 'hubiese', 'eran', 'estos', 'hasta', 'había', 'su', 'sean', 'el', 'fueras', 'tienes', 'mío', 'fuesen', 'fuimos', 'tendrías', 'habrán', 'donde', 'seríamos', 'tenían', 'mías', 'fueseis', 'tendrá', 'otros', 'estuvieses', 'tenéis', 'él', 'mucho', 'eras', 'para', 'sobre', 'fuese', 'teniendo', 'hubieran', 'del', 'uno', 'suyas', 'sea', 'habríais', 'estuviéramos', 'tendréis', 'fueses', 'somos', 'fuéramos', 'estuvieseis', 'sintiendo', 'ese', 'estuve', 'seáis', 'sentidas', 'tendré', 'estarás', 'tiene', 'hemos', 'esta', 'de', 'estuvieras', 'habías', 'habido', 'fueran', 'tuvieran', 'este', 'tuviese', 'hayas', 'por', 'qué', 'mi', 'habiendo', 'nos', 'ante', 'hubierais', 'e', 'que', 'quien', 'tuya', 'estoy', 'estaríais', 'hayamos', 'tendrán', 'estuvieron', 'sentido', 'son', 'suya', 'eso'}\n"
     ]
    }
   ],
   "source": [
    "print(len(stopWords))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py:183: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py:183: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py:183: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py:183: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py:183: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py:183: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py:183: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py:183: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el modelo en español para dividir el texto y obtener formas normales (stemms)\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Y cargamos una lista de palabras vacías en español\n",
    "\n",
    "es_stop = stopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los documentos en formato lista de strings, vamos a obtener el corpus en el formato que neecesitamos. Vamos a crear el corpus como una lista. Dentro de esa lista, cada documento será una lista de strings. Cada string será un término del documento. Para que los términos no nos distorsionen el análisis LDA, vamos a poner todos los términos en minúsculas, pasamos a formas normales y eliminamos palabras vacías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista que incorporará el corpus completo\n",
    "\n",
    "textos = []\n",
    "palabras = []\n",
    "\n",
    "# Y ahora un bucle que pasa por cada documento\n",
    "for doc in doc_list:\n",
    "    \n",
    "    # Pasamos el documento a minusculas\n",
    "    doc_min = doc.lower()\n",
    "    \n",
    "    # Dividimos en palabras y sacamos formas normales con spaCy\n",
    "    doc = nlp(doc_min)\n",
    "    \n",
    "    # Y ahora añadimos la palabra, pero en su forma normal. Sólo añadimos la palabra si no es una palabra vacía.\n",
    "    # Además, si la palabra sólo tiene 1 caracter, no la añado. Probablemente sea un signo de puntuación o una palabra vacía\n",
    "    \n",
    "    for token in doc:\n",
    "        if (not token.lemma_ in es_stop) and (len(token.lemma_)>1):\n",
    "            palabras.append(token.lemma_)\n",
    "    \n",
    "    # Y por último añadimos a los textos el documento ya tratado (formas normales, no palabras vacías...)\n",
    "    \n",
    "    textos.append(palabras)\n",
    "    palabras = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Terminadas las semifinales de la liga ACB de baloncesto, los play-off arrancarán el próximo sábado en Madrid,', 'Una canasta en los últimos momentos dio un resultado inesperado en las finales de la temporada de baloncesto en Estados Unidos', 'La ACB se reunirá para estudiar la conveniencia de aplicar en las canchas de baloncesto las reglas de la NBA, entre ellas alejar a mayor distancia de la canasta la línea de triple. ', 'El mercado de fichajes en las canchas de baloncesto para la próxima temporada ya está en auge. Los diferentes equipos se intentan reforzar para una temporada que presentará cambios en el número de partidos.', 'Polémica en la liga de baloncesto griega. Los dos principales equipos, Panathinaikos y Olympiacos han tenido una temporada con duros enfrentamientos, hasta el punto de que Olympiacos se ha negado a jugar el derbi, y el Panathinaikos gana por 0-20.', 'Vargas Llosa anuncia un libro sobre el golpe militar auspiciado por la CIA en Guatemala en 1954', 'El escritor peruano anunció ayer que este año publicará una novela cuyo título hace alusión «a una cita de Santa Teresa de Ávila»: «Tiempos recios», con edición de Alfaguara.', 'Los libros que José Luis Garcías compró en su visita privada a la Feria del Libro de Madrid', 'Entre lo libros más vendidos de la presente Feria se encuentra sin duda el último ejemplar firmado por Santiago Posteguillo.', 'Muere Agustina Bessa-Luís, una de las grandes escritoras de Portugal. Se hizo conocida para el gran público luso a raíz de su recopilación de las costumbres del Portugal de los siglos XIX y XX, recogidas en su libro Sibila.\\n', '¿Está tu ordenador afectado por Spectre y Meltdown? Lo que necesitas saber', 'Nuevos periféricos para los ordenadores de alta gama de Apple.', '¿Cuál es el ordenador portátil más potente? Comparativa de Surface Book 2 de Microsoft frente a las ofertas de Apple', 'Dell quiere atacar las principales preocupaciones empresariales de TI con sus nuevos modelos de ordenadores de sobremesa.', '¿Tablet u ordenador portátil de última generación? Analizamos las principales diferencias entre dos mundos que se acercan.']\n"
     ]
    }
   ],
   "source": [
    "print(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['terminar', 'semifinal', 'ligar', 'acb', 'baloncesto', 'play', 'off', 'arrancar', 'próximo', 'sábado', 'madrid'], ['canasta', 'último', 'momento', 'dar', 'resultar', 'inesperado', 'final', 'temporada', 'baloncesto', 'unir'], ['acb', 'reunir', 'parir', 'estudiar', 'conveniencia', 'aplicar', 'cancha', 'baloncesto', 'reglar', 'nba', 'entrar', 'alejar', 'mayor', 'distanciar', 'canasta', 'líneo', 'triple'], ['mercar', 'fichaje', 'cancha', 'baloncesto', 'parir', 'próximo', 'temporada', 'auge', 'diferente', 'equipo', 'intentar', 'reforzar', 'parir', 'temporada', 'presentar', 'cambio', 'número', 'partir'], ['polémico', 'ligar', 'baloncesto', 'griego', 'do', 'principal', 'equipo', 'panathinaikos', 'olympiacos', 'haber', 'tener', 'temporada', 'duro', 'enfrentamiento', 'punto', 'olympiacos', 'haber', 'negar', 'jugar', 'derbi', 'panathinaikos', 'ganar', '20'], ['varga', 'llosa', 'anunciar', 'librar', 'sobrar', 'golpe', 'militar', 'auspiciar', 'cia', 'guatemala', '1954'], ['escritor', 'peruano', 'anunciar', 'ayer', 'año', 'publicar', 'novelar', 'cuyo', 'título', 'hacer', 'alusión', 'citar', 'santo', 'teresa', 'ávila', 'tiempo', 'recio', 'edición', 'alfaguara'], ['libro', 'josé', 'luis', 'garcía', 'comprar', 'visitar', 'privar', 'feriar', 'librar', 'madrid'], ['entrar', 'libro', 'vender', 'presentar', 'feriar', 'encontrar', 'dudar', 'último', 'ejemplar', 'firmar', 'santiago', 'posteguillo'], ['morir', 'agustina', 'bessa', 'luir', 'grande', 'escritor', 'portugal', 'hacer', 'conocido', 'parir', 'gran', 'público', 'luso', 'raíz', 'recopilación', 'costumbre', 'portugal', 'siglo', 'xix', 'xx', 'recogida', 'librar', 'sibila'], ['ordenador', 'afectar', 'spectre', 'meltdown', 'necesitar', 'saber'], ['nuevo', 'periférico', 'parir', 'ordenador', 'alto', 'gama', 'apple'], ['cuál', 'ser', 'ordenador', 'portátil', 'potente', 'comparativo', 'surface', 'book', 'microsoft', 'frente', 'ofertar', 'apple'], ['dell', 'querer', 'atacar', 'principal', 'preocupación', 'empresarial', 'nuevo', 'modelo', 'ordenador', 'sobremesa'], ['tablet', 'ordenador', 'portátil', 'último', 'generación', 'analizar', 'principal', 'diferenciar', 'entrar', 'do', 'mundo', 'acercar']]\n"
     ]
    }
   ],
   "source": [
    "print(textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De cara a realizar el análisis LDA, obtenemos el diccionario de los términos de los documentos, así como la matriz de términos documentos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construímos el diccionario, que es una lista de las palabras que aparecen en el corpus textos\n",
    "\n",
    "dictionary = corpora.Dictionary(textos)\n",
    "    \n",
    "# Ahora generamos la matriz términos documentos que nos hace falta para llamar a la función que genera el modelo LDA\n",
    "corpus = [dictionary.doc2bow(texto) for texto in textos]\n",
    "\n",
    "# Y por último generamos el modelo LDA\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=7, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Generamos el modelo LDA y realizamos el análisis. Librería gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a imprimir los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.037*\"ordenador\" + 0.020*\"libro\" + 0.020*\"feriar\" + 0.020*\"santiago\" + 0.020*\"firmar\" + 0.020*\"encontrar\" + 0.020*\"ejemplar\" + 0.020*\"dudar\" + 0.020*\"vender\" + 0.020*\"posteguillo\"'), (1, '0.046*\"parir\" + 0.046*\"baloncesto\" + 0.031*\"temporada\" + 0.031*\"cancha\" + 0.031*\"próximo\" + 0.031*\"acb\" + 0.017*\"canasta\" + 0.017*\"presentar\" + 0.017*\"cambio\" + 0.017*\"auge\"'), (2, '0.047*\"portugal\" + 0.025*\"librar\" + 0.025*\"parir\" + 0.025*\"hacer\" + 0.025*\"escritor\" + 0.025*\"xix\" + 0.025*\"siglo\" + 0.025*\"recopilación\" + 0.025*\"agustina\" + 0.025*\"xx\"'), (3, '0.033*\"olympiacos\" + 0.033*\"panathinaikos\" + 0.033*\"haber\" + 0.018*\"ligar\" + 0.018*\"anunciar\" + 0.018*\"equipo\" + 0.018*\"griego\" + 0.018*\"enfrentamiento\" + 0.018*\"jugar\" + 0.018*\"teresa\"'), (4, '0.033*\"último\" + 0.033*\"ordenador\" + 0.033*\"entrar\" + 0.033*\"portátil\" + 0.033*\"principal\" + 0.033*\"do\" + 0.033*\"tablet\" + 0.033*\"acercar\" + 0.033*\"analizar\" + 0.033*\"mundo\"'), (5, '0.035*\"nuevo\" + 0.035*\"ordenador\" + 0.035*\"principal\" + 0.035*\"modelo\" + 0.035*\"sobremesa\" + 0.035*\"querer\" + 0.035*\"preocupación\" + 0.035*\"empresarial\" + 0.035*\"dell\" + 0.035*\"atacar\"'), (6, '0.021*\"apple\" + 0.021*\"madrid\" + 0.021*\"book\" + 0.021*\"frente\" + 0.021*\"potente\" + 0.021*\"ser\" + 0.021*\"cuál\" + 0.021*\"comparativo\" + 0.021*\"microsoft\" + 0.021*\"surface\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=7, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librería de visualización pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis nos permite obtener los datos obtenidos en el ançalisis LDA de forma gráfica. Una vez realizado el análisis, habilitamos que los resultados se puedan visualizar en el propio notebook con pyLDAvis.enable_notebook(), y preparamos los datos. Los atributos de la función prepare son el resultado del análisis (ldamodel), el corpus y el diccionario.\n",
    "Por último, los datos preparados se pueden visualizar de forma gráfica con la función display. Es una visualización interactiva, cada vez que se pulsa uno de los tópicos, se representan las palabras que lo componen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Knowhow\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook() #Para visualizar los datos obtenidos en la práctica\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1084022223453137606122443336\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1084022223453137606122443336_data = {\"mdsDat\": {\"x\": [-0.04868106506055004, -0.1440208050288533, 0.11686024449593857, 0.06921314935057232, -0.00993403964742106, 0.010770316381834443, 0.005792199508479234], \"y\": [0.16967592432760717, -0.07783689104340123, -0.03131814546871081, 0.010480049349471027, -0.031221697759312463, -0.019161135967758144, -0.020618103437895628], \"topics\": [1, 2, 3, 4, 5, 6, 7], \"cluster\": [1, 1, 1, 1, 1, 1, 1], \"Freq\": [22.47083282470703, 20.928306579589844, 17.115243911743164, 15.538585662841797, 11.948382377624512, 6.493400573730469, 5.505252361297607]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\"], \"Freq\": [3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.4143869876861572, 1.4143869876861572, 1.4143868684768677, 0.7543351650238037, 0.7543351650238037, 0.7543351650238037, 0.7543351650238037, 0.7543351650238037, 0.7543351650238037, 0.7543351650238037, 0.7543351650238037, 0.7543351650238037, 0.754335343837738, 0.754335343837738, 0.754335343837738, 0.754335343837738, 0.754335343837738, 0.754335343837738, 0.754335343837738, 0.754335343837738, 0.7543351650238037, 0.754335343837738, 0.7543351650238037, 0.7543348073959351, 0.7543348073959351, 0.7543348073959351, 0.7543348073959351, 0.7543348073959351, 0.7543348073959351, 2.0746426582336426, 2.0745861530303955, 1.4144678115844727, 0.7543386220932007, 0.7543380856513977, 1.3990932703018188, 1.3990931510925293, 1.3990930318832397, 0.7461788654327393, 0.7461787462234497, 0.7461788654327393, 0.7461787462234497, 0.7461788654327393, 0.7461788654327393, 0.7461788654327393, 0.7461788654327393, 0.7461788654327393, 0.7461787462234497, 0.7461788654327393, 0.7461787462234497, 0.7461788654327393, 0.7461787462234497, 0.7461788654327393, 0.7461788654327393, 0.7461787462234497, 0.7461787462234497, 0.7461788654327393, 0.7461787462234497, 0.7461788654327393, 0.7461788654327393, 0.7461786866188049, 0.7461788654327393, 0.7461788654327393, 0.7461788654327393, 0.7461786866188049, 0.7462030649185181, 0.7461894154548645, 0.7461864352226257, 0.6729007363319397, 0.6729007363319397, 0.6729007363319397, 0.6729007363319397, 0.6729007363319397, 0.6729007363319397, 0.6729007363319397, 0.6729007363319397, 0.6729007363319397, 0.6729007959365845, 0.6729007959365845, 0.6729007959365845, 0.6729007959365845, 0.6729007959365845, 0.6729007959365845, 0.6729007959365845, 0.6729002594947815, 0.6729002594947815, 0.6729002594947815, 0.6729000806808472, 0.6729000806808472, 0.6729000806808472, 0.6729000806808472, 0.6729000806808472, 0.672818660736084, 1.2616288661956787, 0.6729024052619934, 0.6729024052619934, 0.6728696823120117, 0.67289137840271, 0.6728982925415039, 0.6558060050010681, 0.6558060050010681, 0.6558060050010681, 0.6558060050010681, 0.6558060050010681, 0.6558060050010681, 0.6558060050010681, 0.6558060050010681, 0.6558060050010681, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.6558059453964233, 0.655775785446167, 0.6558359265327454, 0.6558042764663696, 0.6558042764663696, 0.6558191776275635, 0.6558029055595398, 0.6557641625404358, 0.6557753086090088, 0.6557283401489258, 0.6557419896125793, 1.1328363418579102, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041770577430725, 0.6041923761367798, 0.6041923761367798, 0.6042359471321106, 0.6042163968086243, 0.07552269846200943, 0.07552269846200943, 0.07552269846200943, 0.07552269846200943, 0.07552269846200943, 0.07552269846200943, 0.07552269846200943, 0.07552269846200943, 0.07552280277013779, 0.0755227729678154, 0.0755227729678154, 0.0755227655172348, 0.07552275061607361, 0.07552275061607361, 0.07552275061607361, 0.07552275061607361, 0.4332449436187744, 0.4332449436187744, 0.4332449436187744, 0.4332449436187744, 0.4332449436187744, 0.4332449436187744, 0.43326494097709656, 0.43325403332710266, 0.4332556426525116, 0.4332951307296753, 0.433287113904953, 0.4332936108112335, 0.054155945777893066, 0.054155945777893066, 0.054155945777893066, 0.054155945777893066, 0.054155945777893066, 0.054155945777893066, 0.054155945777893066, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.05415594205260277, 0.054156020283699036, 0.05415600910782814, 0.05415600165724754, 0.054155994206666946, 0.054155994206666946, 0.054155994206666946, 0.054155994206666946, 0.054155994206666946, 0.054155994206666946, 0.054155994206666946, 0.05415601283311844, 0.05415601283311844, 0.05415600910782814, 0.05415600910782814, 0.05415598303079605, 0.05415598303079605, 0.05415598303079605, 0.05415598303079605, 0.38997069001197815, 0.38997069001197815, 0.38997069001197815, 0.38997069001197815, 0.38997069001197815, 0.38997069001197815, 0.38997069001197815, 0.39001813530921936, 0.3899773061275482, 0.39001181721687317, 0.04874662309885025, 0.04874662309885025, 0.04874662309885025, 0.04874662309885025, 0.04874662309885025, 0.04874662309885025, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.048746611922979355, 0.04874669015407562, 0.048746686428785324, 0.048746686428785324, 0.04874667897820473, 0.04874667897820473, 0.04874667897820473, 0.048746686428785324, 0.04874667525291443, 0.04874667152762413, 0.04874667152762413, 0.048746660351753235, 0.048746660351753235, 0.048746660351753235, 0.04874667152762413, 0.048746660351753235, 0.04874667152762413, 0.04874666407704353, 0.04874667152762413, 0.048746660351753235, 0.04874667152762413], \"Term\": [\"ordenador\", \"principal\", \"portugal\", \"nuevo\", \"\\u00faltimo\", \"port\\u00e1til\", \"do\", \"entrar\", \"parir\", \"modelo\", \"dell\", \"empresarial\", \"atacar\", \"preocupaci\\u00f3n\", \"sobremesa\", \"querer\", \"diferenciar\", \"mundo\", \"generaci\\u00f3n\", \"tablet\", \"analizar\", \"acercar\", \"olympiacos\", \"panathinaikos\", \"haber\", \"baloncesto\", \"librar\", \"hacer\", \"escritor\", \"pr\\u00f3ximo\", \"cancha\", \"pr\\u00f3ximo\", \"acb\", \"conveniencia\", \"aplicar\", \"estudiar\", \"l\\u00edneo\", \"mayor\", \"nba\", \"reglar\", \"reunir\", \"triple\", \"auge\", \"cambio\", \"diferente\", \"fichaje\", \"intentar\", \"mercar\", \"n\\u00famero\", \"partir\", \"distanciar\", \"reforzar\", \"alejar\", \"terminar\", \"play\", \"arrancar\", \"semifinal\", \"s\\u00e1bado\", \"off\", \"parir\", \"baloncesto\", \"temporada\", \"canasta\", \"presentar\", \"olympiacos\", \"panathinaikos\", \"haber\", \"enfrentamiento\", \"tiempo\", \"teresa\", \"santo\", \"peruano\", \"novelar\", \"cuyo\", \"citar\", \"a\\u00f1o\", \"ayer\", \"alusi\\u00f3n\", \"alfaguara\", \"ganar\", \"edici\\u00f3n\", \"duro\", \"20\", \"t\\u00edtulo\", \"derbi\", \"negar\", \"\\u00e1vila\", \"griego\", \"pol\\u00e9mico\", \"publicar\", \"punto\", \"tener\", \"jugar\", \"recio\", \"ligar\", \"anunciar\", \"equipo\", \"llosa\", \"militar\", \"sobrar\", \"varga\", \"1954\", \"auspiciar\", \"cia\", \"golpe\", \"guatemala\", \"vender\", \"dudar\", \"ejemplar\", \"encontrar\", \"firmar\", \"posteguillo\", \"santiago\", \"alto\", \"gama\", \"perif\\u00e9rico\", \"saber\", \"spectre\", \"necesitar\", \"afectar\", \"meltdown\", \"nuevo\", \"ordenador\", \"feriar\", \"libro\", \"apple\", \"anunciar\", \"presentar\", \"cu\\u00e1l\", \"microsoft\", \"frente\", \"potente\", \"ser\", \"surface\", \"comparativo\", \"book\", \"ofertar\", \"dar\", \"inesperado\", \"comprar\", \"garc\\u00eda\", \"jos\\u00e9\", \"luis\", \"privar\", \"visitar\", \"final\", \"unir\", \"resultar\", \"momento\", \"port\\u00e1til\", \"apple\", \"libro\", \"feriar\", \"madrid\", \"canasta\", \"\\u00faltimo\", \"librar\", \"ordenador\", \"temporada\", \"portugal\", \"p\\u00fablico\", \"xx\", \"siglo\", \"sibila\", \"recopilaci\\u00f3n\", \"recogida\", \"ra\\u00edz\", \"morir\", \"luso\", \"luir\", \"grande\", \"gran\", \"costumbre\", \"conocido\", \"bessa\", \"agustina\", \"xix\", \"hacer\", \"escritor\", \"librar\", \"parir\", \"atacar\", \"dell\", \"empresarial\", \"modelo\", \"preocupaci\\u00f3n\", \"querer\", \"sobremesa\", \"mundo\", \"ordenador\", \"\\u00faltimo\", \"apple\", \"baloncesto\", \"afectar\", \"entrar\", \"libro\", \"feriar\", \"tablet\", \"mundo\", \"generaci\\u00f3n\", \"diferenciar\", \"analizar\", \"acercar\", \"port\\u00e1til\", \"do\", \"principal\", \"\\u00faltimo\", \"entrar\", \"ordenador\", \"dell\", \"empresarial\", \"modelo\", \"preocupaci\\u00f3n\", \"querer\", \"sobremesa\", \"atacar\", \"luso\", \"sibila\", \"recopilaci\\u00f3n\", \"recogida\", \"ra\\u00edz\", \"morir\", \"costumbre\", \"luir\", \"grande\", \"gran\", \"conocido\", \"p\\u00fablico\", \"parir\", \"nuevo\", \"temporada\", \"afectar\", \"libro\", \"meltdown\", \"necesitar\", \"feriar\", \"saber\", \"spectre\", \"apple\", \"librar\", \"baloncesto\", \"madrid\", \"perif\\u00e9rico\", \"gama\", \"alto\", \"presentar\", \"dell\", \"querer\", \"atacar\", \"empresarial\", \"modelo\", \"preocupaci\\u00f3n\", \"sobremesa\", \"nuevo\", \"principal\", \"ordenador\", \"acercar\", \"analizar\", \"diferenciar\", \"generaci\\u00f3n\", \"mundo\", \"tablet\", \"morir\", \"siglo\", \"sibila\", \"recopilaci\\u00f3n\", \"recogida\", \"p\\u00fablico\", \"bessa\", \"luso\", \"luir\", \"grande\", \"gran\", \"costumbre\", \"conocido\", \"xx\", \"ra\\u00edz\", \"xix\", \"parir\", \"apple\", \"librar\", \"entrar\", \"madrid\", \"baloncesto\", \"\\u00faltimo\", \"temporada\", \"spectre\", \"saber\", \"alto\", \"perif\\u00e9rico\", \"gama\", \"meltdown\", \"anunciar\", \"afectar\", \"canasta\", \"feriar\", \"presentar\", \"libro\"], \"Total\": [3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.8521755933761597, 1.8521755933761597, 1.8521754741668701, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921238899230957, 1.1921238899230957, 1.1921238899230957, 1.1921238899230957, 1.1921238899230957, 1.1921238899230957, 1.1921238899230957, 1.1921238899230957, 1.1921236515045166, 1.1921238899230957, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 1.1921236515045166, 3.629685640335083, 3.738978147506714, 3.07892107963562, 1.7659538984298706, 1.7809116840362549, 1.8379020690917969, 1.8379021883010864, 1.8379021883010864, 1.1849873065948486, 1.184987187385559, 1.1849873065948486, 1.184987187385559, 1.1849873065948486, 1.1849873065948486, 1.1849873065948486, 1.1849873065948486, 1.1849873065948486, 1.184987187385559, 1.1849873065948486, 1.184987187385559, 1.1849873065948486, 1.184987187385559, 1.1849873065948486, 1.1849873065948486, 1.184987187385559, 1.1849873065948486, 1.1849874258041382, 1.1849873065948486, 1.1849874258041382, 1.1849874258041382, 1.184987187385559, 1.1849874258041382, 1.1849874258041382, 1.1849874258041382, 1.1849873065948486, 1.845029354095459, 1.7737759351730347, 1.8450298309326172, 1.1208688020706177, 1.1208688020706177, 1.1208688020706177, 1.1208688020706177, 1.1208688020706177, 1.1208688020706177, 1.1208688020706177, 1.1208688020706177, 1.1208688020706177, 1.1208689212799072, 1.1208689212799072, 1.1208689212799072, 1.1208689212799072, 1.1208689212799072, 1.1208689212799072, 1.1208689212799072, 1.1208685636520386, 1.1208685636520386, 1.1208685636520386, 1.120868444442749, 1.120868444442749, 1.120868444442749, 1.120868444442749, 1.120868444442749, 1.4620585441589355, 3.0037519931793213, 1.694698452949524, 1.694698452949524, 1.694697618484497, 1.7737759351730347, 1.7809116840362549, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.1059110164642334, 1.4849900007247925, 1.694697618484497, 1.694698452949524, 1.694698452949524, 1.7659512758255005, 1.7659538984298706, 2.073760986328125, 2.2233471870422363, 3.0037519931793213, 3.07892107963562, 1.5893951654434204, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.713638186454773, 1.7136380672454834, 2.2233471870422363, 3.629685640335083, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.9111705422401428, 3.0037519931793213, 2.073760986328125, 1.694697618484497, 3.738978147506714, 1.120868444442749, 2.1599738597869873, 1.694698452949524, 1.694698452949524, 0.9111705422401428, 0.9111705422401428, 0.9111705422401428, 0.9111705422401428, 0.9111705422401428, 0.9111705422401428, 1.4849900007247925, 1.5640698671340942, 1.9052870273590088, 2.073760986328125, 2.1599738597869873, 3.0037519931793213, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 3.629685640335083, 1.4620585441589355, 3.07892107963562, 1.120868444442749, 1.694698452949524, 1.120868444442749, 1.120868444442749, 1.694698452949524, 1.120868444442749, 1.120868444442749, 1.694697618484497, 2.2233471870422363, 3.738978147506714, 1.7659512758255005, 1.1208685636520386, 1.1208685636520386, 1.1208685636520386, 1.7809116840362549, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 0.8733055591583252, 1.4620585441589355, 1.9052870273590088, 3.0037519931793213, 0.9111705422401428, 0.9111705422401428, 0.9111705422401428, 0.9111705422401428, 0.9111705422401428, 0.9111705422401428, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 1.060735821723938, 3.629685640335083, 1.694697618484497, 2.2233471870422363, 2.1599738597869873, 1.7659512758255005, 3.738978147506714, 2.073760986328125, 3.07892107963562, 1.120868444442749, 1.120868444442749, 1.1208685636520386, 1.1208685636520386, 1.1208685636520386, 1.120868444442749, 1.7737759351730347, 1.120868444442749, 1.7659538984298706, 1.694698452949524, 1.7809116840362549, 1.694698452949524], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2232999801635742, 1.2232999801635742, 1.2232999801635742, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 1.0353000164031982, 0.9336000084877014, 0.9039000272750854, 0.7150999903678894, 0.642300009727478, 0.633899986743927, 1.2913000583648682, 1.2913000583648682, 1.2913000583648682, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 1.1015000343322754, 0.6588000059127808, 0.698199987411499, 0.6588000059127808, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 1.2548999786376953, 0.9890999794006348, 0.8977000117301941, 0.8414999842643738, 0.8414999842643738, 0.8414999842643738, 0.7958999872207642, 0.7918999791145325, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.3393000364303589, 1.0444999933242798, 0.9125000238418579, 0.9124000072479248, 0.9124000072479248, 0.8712999820709229, 0.8712999820709229, 0.7105000019073486, 0.6409000158309937, 0.3400000035762787, 0.31529998779296875, 1.7858999967575073, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.5616999864578247, 1.0821000337600708, 1.0821000337600708, 0.8217999935150146, 0.33160001039505005, -0.32330000400543213, -0.32330000400543213, -0.32330000400543213, -0.32330000400543213, -0.32330000400543213, -0.32330000400543213, -0.32330000400543213, -0.36570000648498535, -1.5585999488830566, -1.188099980354309, -0.986299991607666, -1.7776000499725342, -0.5728999972343445, -1.2288000583648682, -0.986299991607666, -0.986299991607666, 1.9910000562667847, 1.9910000562667847, 1.9910000562667847, 1.9910000562667847, 1.9910000562667847, 1.9910000562667847, 1.5025999546051025, 1.450700044631958, 1.2532999515533447, 1.1686999797821045, 1.1279000043869019, 0.7982000112533569, -0.04600000008940697, -0.04600000008940697, -0.04600000008940697, -0.04600000008940697, -0.04600000008940697, -0.04600000008940697, -0.04600000008940697, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -0.24050000309944153, -1.4706000089645386, -0.5612999796867371, -1.3061000108718872, -0.2955999970436096, -0.7089999914169312, -0.2955999970436096, -0.2955999970436096, -0.7089999914169312, -0.2955999970436096, -0.2955999970436096, -0.7089999914169312, -0.9804999828338623, -1.5003000497817993, -0.7501999735832214, -0.2955999970436096, -0.2955999970436096, -0.2955999970436096, -0.7585999965667725, 2.0933001041412354, 2.0933001041412354, 2.0933001041412354, 2.0933001041412354, 2.0933001041412354, 2.0933001041412354, 2.0933001041412354, 1.5780999660491943, 1.3131999969482422, 0.8579999804496765, -0.028599999845027924, -0.028599999845027924, -0.028599999845027924, -0.028599999845027924, -0.028599999845027924, -0.028599999845027924, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -0.18060000240802765, -1.4107999801635742, -0.6492000222206116, -0.9207000136375427, -0.891700029373169, -0.6902999877929688, -1.440500020980835, -0.8510000109672546, -1.2461999654769897, -0.23579999804496765, -0.23579999804496765, -0.23579999804496765, -0.23579999804496765, -0.23579999804496765, -0.23579999804496765, -0.6948000192642212, -0.23579999804496765, -0.6902999877929688, -0.6492000222206116, -0.6988000273704529, -0.6492000222206116], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.46370005607605, -3.46370005607605, -3.46370005607605, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -4.092299938201904, -3.0806000232696533, -3.0806000232696533, -3.463599920272827, -4.092299938201904, -4.092299938201904, -3.40339994430542, -3.40339994430542, -3.40339994430542, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -4.0320000648498535, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.9344000816345215, -3.3057000637054443, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.934299945831299, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.863300085067749, -3.8633999824523926, -3.8633999824523926, -3.863300085067749, -3.8633999824523926, -3.8633999824523926, -3.8633999824523926, -3.8635001182556152, -3.8633999824523926, -3.053999900817871, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.6826000213623047, -3.682499885559082, -3.6826000213623047, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -5.7621002197265625, -3.405400037765503, -3.405400037765503, -3.405400037765503, -3.405400037765503, -3.405400037765503, -3.405400037765503, -3.4052999019622803, -3.405400037765503, -3.4052999019622803, -3.4052999019622803, -3.4052999019622803, -3.4052999019622803, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -5.484799861907959, -3.3454999923706055, -3.3454999923706055, -3.3454999923706055, -3.3454999923706055, -3.3454999923706055, -3.3454999923706055, -3.3454999923706055, -3.345400094985962, -3.3454999923706055, -3.345400094985962, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863, -5.425000190734863]}, \"token.table\": {\"Topic\": [3, 2, 1, 3, 5, 1, 2, 3, 2, 2, 3, 1, 3, 4, 1, 1, 3, 2, 2, 1, 2, 4, 5, 4, 1, 1, 4, 1, 3, 2, 4, 4, 5, 1, 5, 2, 4, 4, 2, 1, 1, 2, 3, 2, 2, 3, 3, 2, 1, 3, 1, 2, 2, 5, 1, 3, 4, 1, 4, 3, 4, 3, 2, 4, 3, 5, 5, 2, 3, 2, 2, 5, 4, 1, 4, 2, 3, 4, 5, 3, 4, 1, 2, 3, 5, 4, 5, 1, 1, 4, 1, 3, 1, 4, 3, 4, 5, 1, 3, 2, 2, 3, 1, 4, 1, 2, 3, 4, 2, 1, 3, 5, 1, 3, 2, 1, 2, 5, 4, 3, 4, 1, 3, 2, 4, 1, 2, 2, 5, 5, 2, 5, 5, 1, 1, 4, 1, 3, 3, 2, 1, 4, 5, 5, 3, 3, 4, 1, 1, 2, 4, 2, 2, 1, 2, 1, 2, 4, 3, 3, 4, 5, 5, 2, 3, 4], \"Freq\": [0.8921650648117065, 0.843890905380249, 0.5399056673049927, 0.8921653628349304, 0.9427418112754822, 0.8388391733169556, 0.8438909649848938, 0.8921652436256409, 0.843890905380249, 0.5637690424919128, 0.5637690424919128, 0.8388391733169556, 0.5900757908821106, 0.5900757908821106, 0.8388391733169556, 0.8388389945030212, 0.8921650648117065, 0.8438909649848938, 0.843890905380249, 0.5349054932594299, 0.26745274662971497, 0.26745274662971497, 0.9427418112754822, 0.9042319059371948, 0.8388389945030212, 0.5662661790847778, 0.5662661790847778, 0.5399056077003479, 0.8921650648117065, 0.843890905380249, 0.9042319059371948, 0.9042319059371948, 0.9427418112754822, 0.8388391733169556, 0.9427418112754822, 0.843890905380249, 0.9042319059371948, 0.9042319059371948, 0.843890905380249, 0.8388389945030212, 0.8388391733169556, 0.6393576264381409, 0.8921650052070618, 0.843890905380249, 0.8438909649848938, 0.8921650052070618, 0.8921650052070618, 0.843890905380249, 0.4629685580730438, 0.4629685580730438, 0.5419966578483582, 0.5419966578483582, 0.5835537910461426, 0.5835537910461426, 0.8388391733169556, 0.5900754928588867, 0.5900754928588867, 0.8388389945030212, 0.9042319059371948, 0.8921650052070618, 0.9042319059371948, 0.8921652436256409, 0.843890905380249, 0.9042319059371948, 0.8921650648117065, 0.9427418112754822, 0.9427418112754822, 0.8438907861709595, 0.8921650648117065, 0.5440986156463623, 0.5835537314414978, 0.5835537314414978, 0.9042319059371948, 0.8388389945030212, 0.9042319059371948, 0.8438907861709595, 0.44977229833602905, 0.44977229833602905, 0.44977229833602905, 0.5900754928588867, 0.5900754928588867, 0.5419967770576477, 0.5419967770576477, 0.8921650648117065, 0.9427418112754822, 0.9042319059371948, 0.9427418112754822, 0.8388391733169556, 0.5662670135498047, 0.5662670135498047, 0.8388391733169556, 0.8921653628349304, 0.8388389945030212, 0.9042319059371948, 0.8921650648117065, 0.9042319059371948, 0.9427418112754822, 0.8388391733169556, 0.8921653628349304, 0.8438907861709595, 0.843890905380249, 0.683967113494873, 0.8388389945030212, 0.9042319059371948, 0.8388391733169556, 0.5440986156463623, 0.3329169750213623, 0.3329169750213623, 0.5440986156463623, 0.5510119199752808, 0.2755059599876404, 0.2755059599876404, 0.8388389945030212, 0.8921652436256409, 0.843890905380249, 0.8388391733169556, 0.8438907861709595, 0.6291701793670654, 0.6734052300453186, 0.8921650052070618, 0.9042319059371948, 0.5615101456642151, 0.5615101456642151, 0.5248553156852722, 0.9042319059371948, 0.5399056077003479, 0.8438909649848938, 0.8438907861709595, 0.9427418112754822, 0.9427418112754822, 0.843890905380249, 0.9427418112754822, 0.9427418112754822, 0.8388389945030212, 0.8388391733169556, 0.9042319059371948, 0.8388391733169556, 0.8921653628349304, 0.8921650052070618, 0.8438909649848938, 0.8388391733169556, 0.9042319059371948, 0.9427418112754822, 0.9427418112754822, 0.8921650648117065, 0.8921653628349304, 0.9042319059371948, 0.8388391733169556, 0.3247891068458557, 0.3247891068458557, 0.3247891068458557, 0.8438907861709595, 0.843890905380249, 0.8388391733169556, 0.8438909649848938, 0.8388391733169556, 0.8438909649848938, 0.9042319059371948, 0.8921650648117065, 0.8921650052070618, 0.9042319059371948, 0.9427418112754822, 0.9427418112754822, 0.843890905380249, 0.48221564292907715, 0.48221564292907715], \"Term\": [\"1954\", \"20\", \"acb\", \"afectar\", \"agustina\", \"alejar\", \"alfaguara\", \"alto\", \"alusi\\u00f3n\", \"anunciar\", \"anunciar\", \"aplicar\", \"apple\", \"apple\", \"arrancar\", \"auge\", \"auspiciar\", \"ayer\", \"a\\u00f1o\", \"baloncesto\", \"baloncesto\", \"baloncesto\", \"bessa\", \"book\", \"cambio\", \"canasta\", \"canasta\", \"cancha\", \"cia\", \"citar\", \"comparativo\", \"comprar\", \"conocido\", \"conveniencia\", \"costumbre\", \"cuyo\", \"cu\\u00e1l\", \"dar\", \"derbi\", \"diferente\", \"distanciar\", \"do\", \"dudar\", \"duro\", \"edici\\u00f3n\", \"ejemplar\", \"encontrar\", \"enfrentamiento\", \"entrar\", \"entrar\", \"equipo\", \"equipo\", \"escritor\", \"escritor\", \"estudiar\", \"feriar\", \"feriar\", \"fichaje\", \"final\", \"firmar\", \"frente\", \"gama\", \"ganar\", \"garc\\u00eda\", \"golpe\", \"gran\", \"grande\", \"griego\", \"guatemala\", \"haber\", \"hacer\", \"hacer\", \"inesperado\", \"intentar\", \"jos\\u00e9\", \"jugar\", \"librar\", \"librar\", \"librar\", \"libro\", \"libro\", \"ligar\", \"ligar\", \"llosa\", \"luir\", \"luis\", \"luso\", \"l\\u00edneo\", \"madrid\", \"madrid\", \"mayor\", \"meltdown\", \"mercar\", \"microsoft\", \"militar\", \"momento\", \"morir\", \"nba\", \"necesitar\", \"negar\", \"novelar\", \"nuevo\", \"n\\u00famero\", \"ofertar\", \"off\", \"olympiacos\", \"ordenador\", \"ordenador\", \"panathinaikos\", \"parir\", \"parir\", \"parir\", \"partir\", \"perif\\u00e9rico\", \"peruano\", \"play\", \"pol\\u00e9mico\", \"portugal\", \"port\\u00e1til\", \"posteguillo\", \"potente\", \"presentar\", \"presentar\", \"principal\", \"privar\", \"pr\\u00f3ximo\", \"publicar\", \"punto\", \"p\\u00fablico\", \"ra\\u00edz\", \"recio\", \"recogida\", \"recopilaci\\u00f3n\", \"reforzar\", \"reglar\", \"resultar\", \"reunir\", \"saber\", \"santiago\", \"santo\", \"semifinal\", \"ser\", \"sibila\", \"siglo\", \"sobrar\", \"spectre\", \"surface\", \"s\\u00e1bado\", \"temporada\", \"temporada\", \"temporada\", \"tener\", \"teresa\", \"terminar\", \"tiempo\", \"triple\", \"t\\u00edtulo\", \"unir\", \"varga\", \"vender\", \"visitar\", \"xix\", \"xx\", \"\\u00e1vila\", \"\\u00faltimo\", \"\\u00faltimo\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 4, 1, 7, 3, 5, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1084022223453137606122443336\", ldavis_el1084022223453137606122443336_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1084022223453137606122443336\", ldavis_el1084022223453137606122443336_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1084022223453137606122443336\", ldavis_el1084022223453137606122443336_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta visualización en pyLDAvis se puede observar que tenemos un conjunto de documentos, y hemos fijado un número K de tópicos que queremos descubrir. LDA debe crear el modelo y asignar cada documento a uno de los tópicos.\n",
    "\n",
    "Adicionalmente podemos ver queremos las palabras de cada documento asociadas con cada tópico (en este caso son 7 tópicos lo que muestra LDA ya que así se lo marcamos.\n",
    "\n",
    "Por cada palabra w el documento d, asumimos que la asignación de documentos a tópicos es correcta para todo el resto de palabras. Es decir eliminamos que la palabra w indica el tópico t en el documento d.\n",
    "Asignamos la palabra w en el documento d al tópico más habitual de la palabra w, Volvemos a iterar.\n",
    "Se ha comprobado que el sistema se estabiliza al cabo de unas cuantas iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
